\documentclass{article}
\input{new_commands}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}

\newtheorem{theorem}{Theorem} % continuous numbers
%%\newtheorem{theorem}{Theorem}[section] % sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
\newtheorem{lemma}{Lemma}% 
%%\newtheorem{proposition}{Proposition} % to get separate numbers for theorem and proposition etc.

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

%%%


\title{Convergence of the loss function surface in transformer neural network architectures}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Egor Petrov\\
  MIPT\\
  Moscow, Russia\\
  \texttt{petrov.egor.d@phystech.edu}\\
  \And
  Nikita Kiselev\\
  MIPT\\
  Moscow, Russia\\
  \texttt{kiselev.ns@phystech.edu}\\
  \And
  Vladislav Meshkov\\
  MIPT\\
  Moscow, Russia\\
  \texttt{email@phystech.edu}\\
  \And
  Andrey Grabovoy\\
  MIPT\\
  Moscow, Russia\\
  \texttt{email@phystech.edu}\\
}


\begin{document}


\maketitle

\begin{abstract}
    Training a neural network involves searching for the minimum point of the loss function, which defines the surface in the space of model parameters. 
    The properties of this surface are determined by the chosen architecture, the loss function, and the training data. 
    Existing studies show that as the number of objects in the sample increases, the surface of the loss function ceases to change significantly. 
    The paper obtains an estimate for the convergence of the surface of the loss function for the transformer architecture of a neural network with attention layers, as well as conducts computational experiments that confirm the obtained theoretical results. 
    In this paper, we propose a theoretical estimate for the minimum sample size required to train a model with any predetermined acceptable error, providing experiments that prove the theoretical boundaries.
\end{abstract}

\textbf{Keywords:} Neural networks, Transformer, Loss landscape, Hessian, Dataset size threshold.

\section{Introduction}\label{sec:intro}

TODO

\textbf{Contributions.} Our contributions can be summarized as follows:
\begin{itemize}
    \item We present...
    \item We demonstrate the validity of our theoretical results through empirical studies...
    \item We highlight the implications of our findings for...
\end{itemize}

\textbf{Outline.} The rest of the paper is organized as follows...

\section{Related Work}\label{sec:rw}

\textbf{Topic \#1.}
TODO

\textbf{Topic \#2.}
TODO

\section{Preliminaries}\label{sec:prelim}

\subsection{General notation}

In this section, we introduce the general notation used in the rest of the paper and the basic assumptions. 

\subsection{Assumptions} 

TODO

\section{Method}\label{sec:method}

\section{Experiments}\label{sec:exp}

To verify the theoretical estimates obtained, we conducted a detailed empirical study...

\section{Discussion}\label{sec:disc}

TODO

\section{Conclusion}\label{sec:concl}

TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Appendix / supplemental material}\label{app}

\subsection{Additional experiments / Proofs of Theorems}\label{app:exp}

TODO

\end{document}
