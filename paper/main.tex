\documentclass{article}
\input{new_commands}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, sort, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}

\newtheorem{theorem}{Theorem} % continuous numbers
%%\newtheorem{theorem}{Theorem}[section] % sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
\newtheorem{lemma}{Lemma}% 
%%\newtheorem{proposition}{Proposition} % to get separate numbers for theorem and proposition etc.

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

%%%


\title{Convergence of the loss function surface in transformer neural network architectures}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Egor Petrov\\
  MIPT\\
  Moscow, Russia\\
  \texttt{petrov.egor.d@phystech.edu}\\
  \And
  Nikita Kiselev\\
  MIPT\\
  Moscow, Russia\\
  \texttt{kiselev.ns@phystech.edu}\\
  \And
  Vladislav Meshkov\\
  MIPT\\
  Moscow, Russia\\
  \texttt{email@phystech.edu}\\
  \And
  Andrey Grabovoy\\
  MIPT\\
  Moscow, Russia\\
  \texttt{email@phystech.edu}\\
}


\begin{document}


\maketitle

\begin{abstract}
    Training a neural network involves searching for the minimum point of the loss function, which defines the surface in the space of model parameters. 
    The properties of this surface are determined by the chosen architecture, the loss function, and the training data. 
    Existing studies show that as the number of objects in the sample increases, the surface of the loss function ceases to change significantly. 
    The paper obtains an estimate for the convergence of the surface of the loss function for the transformer architecture of a neural network with attention layers, as well as conducts computational experiments that confirm the obtained theoretical results. 
    In this paper, we propose a theoretical estimate for the minimum sample size required to train a model with any predetermined acceptable error, providing experiments that prove the theoretical boundaries.
\end{abstract}

\textbf{Keywords:} Neural networks, Transformer, Loss landscape, Hessian, Dataset size threshold.

\section{Introduction}\label{sec:intro}

Neural networks are becoming more and more popular these days as a way to solve many practical problems. In particular, the Transformer \cite{vaswani2017attention} architecture has revolutionized the world of text processing, as well as many other popular tasks, such as image processing with Visual Transformer \cite{wu2020visualtransformer}. Current results show that as the training sample size increases, there is a trend for the quality of the \cite{kaplan2020scaling} model to increase, but there are many areas, such as, for example, processing specific medical data, where the datasets are significantly limited \cite{moreno2024medicalfewshot} and obtaining new sample objects is costly. Moreover, many models using neural networks and transformers require large computational power and resources. 

In our work, we propose an analysis of the model and loss function, as applied to the Transformer architecture, on the minimum sample size to achieve a predetermined quality set in advance. Based on the approach proposed in the paper \cite{kiselev2024unraveling}, we provide a theoretical analysis of the Transformer architecture by considering the differences between the loss function when adding the next sample item to the dataset. 

We obtain theoretical estimates for the convergence of this difference in a transformer neural
network as the sample size approaches infinity. These results are derived through the analysis of the
Hessian spectrum. These estimates allow us to determine the dependence of this difference on the
structure of the neural network. We
empirically verify these theoretical results by examining the behavior of the loss surface on various
datasets. The obtained plots substantiate the validity of the theoretical calculations.

\textbf{Contributions.} Our contributions can be summarized as follows:
\begin{itemize}
    \item We apply a Hessian-based approach to
        find the critical sufficient dataset size for transformer architectures.
    \item We demonstrate the validity of our theoretical results through empirical studies on the task of image classification using ViT's
    \item We highlight the implications of our findings for practical data collection strategies, showing how the detection of
        a sufficient dataset size can reduce computation time.
\end{itemize}

\textbf{Outline.} TODO REFERENCES The rest of the paper is organized as follows. In Section 2, we review related work, categorizing existing research into key topics and highlighting their main contributions. Section 3 introduces the notation and presents preliminary calculations essential for our analysis. In Section 4, we derive theoretical bounds for the norm of the Hessian matrix and the norm of the difference between loss functions. Section 5 provides an empirical study validating these theoretical results. Sections 6 and 7 discuss and summarize our findings, offering insights and conclusions. Additional experiments and proofs of theorems are included in Appendix A.

\section{Related Work}\label{sec:rw}

\textbf{Geometry of Neural Network Loss Landscapes}

The geometry of neural network loss landscapes, often analyzed via the Hessian matrix, is a well-studied area. \cite{fort2019emergentpropertieslocalgeometry} shows that in multi-label classification, the landscape exhibits exactly $K$ directions of high curvature, where $K$ is the number of classes. \cite{pmlr-v70-pennington17a} use random matrix theory to explore loss surface dynamics and optimization. \cite{garipov2018losssurfacesmodeconnectivity} models linear mode connectivity, demonstrating minima connectivity, while \cite{singh2022phenomenologydoubledescentfinitewidth} explains double descent in finite-width networks. \cite{wang2023instabilitieslargelearningrate} notes landscape flattening under large learning rates. Studies like \cite{draxler2019essentiallynobarriers, garipov2018losssurfacesmodeconnectivity, nguyen2017losssurfacedeepwide} further explore minima connectivity and landscape structure. However, none of these address how landscape geometry stabilizes with increasing dataset size, a gap our work targets.

\textbf{Hessian-Based Analysis and Generalization}

The Hessian matrix is key to understanding convergence, optimization, and generalization. \cite{kiselev2024unraveling} analyzes fully connected networks, showing how the Hessian spectrum reveals convergence smoothness, while \cite{meshkov2024convnets} extends this to convolutional networks, noting the Hessian’s low effective rank. Yet, these works overlook the impact of sample size on the loss landscape, especially for transformers, an area our study addresses.

\textbf{Loss Landscapes in Transformers}

Transformers, introduced by \cite{vaswani2017attention}, are central to modern deep learning. \cite{ormaniec2024attentionhessian} provides a theoretical Hessian analysis of transformers, paralleling methods for fully connected networks. \cite{li2023theoreticalunderstandingshallowvision} studies sample complexity in vision transformers, akin to our sample size focus, while \cite{zhang2025understandinggeneralizationtransformers} analyzes generalization and dynamics. \cite{anonymous2024stagewisedevelopmenttransformers} offers a geometric view of transformer landscapes but ignores minimal sample size. These studies advance transformer landscape understanding but do not explore convergence with increasing dataset size, a gap our work fills.

\textbf{Dataset Size and Loss Landscape Convergence}

Dataset size’s impact on loss landscapes, particularly for transformers, is underexplored. \cite{hoffmann2022training} questions optimal dataset-model size balances, noting computational costs, and \cite{wu2017towards} links sufficient samples to flatter minima. However, the concept of a minimum viable dataset size—where additional data cause negligible landscape changes—lacks theoretical grounding. \cite{xie2024losslens} hints at identifying such thresholds via visualization but offers no framework. Our work extends the Hessian-based analyses of \cite{kiselev2024unraveling, meshkov2024convnets} to transformers, leveraging \cite{ormaniec2024attentionhessian} to derive convergence bounds as a function of sample size, addressing this critical gap.

\section{Preliminaries}\label{sec:prelim}

\subsection{General notation}

In this section, we introduce the general notation used in the rest of the paper and the basic assumptions similar to \cite{} TODO ARTEM'S WORK.

We consider a $K$-label classification problem.
So, let's consider $p(y|x)$ a conditional probability, which maps unobserved variable $x \in \mathcal{X}$ to the corresponding
output $y \in \mathcal{Y}$. We consider $\mathcal{Y}$ is a subspace (or same space) as $\mathbb{R}^K$. Let $f_{\omega}(\cdot)$
be a neural network with a list of parameters $\omega$. Let $\Omega$ be a space of parameters ($\omega \in \Omega$).

Let
$$\mathcal{D} = \left\{ (x_i, y_i) \, | \, i = 1, \dots, m \right\}$$
be a given dataset of size $m$ consists of i.i.d.
samples. Let $l(\cdot, \cdot)$ be a given loss function (e.g. cross-entrophy) where first argument refers to neural network's
result and the second argument refers to the true answer. To simplify, define: $$l_i(\omega) := l(f_{\omega}(x_i), y_i).$$

\begin{definition}
  The empirical loss function for the first $k$ elements is:
  $$\mathcal{L}_k(\omega) = \frac1k \sum\limits_1^k l_i(\omega), \,\, \mathcal{L}(\omega) := \mathcal{L}_m(\omega).$$
\end{definition}

Thus, the difference between losses for neighbouring samplem sizes is:
$$\mathcal{L}_{k}(\omega) - \mathcal{L}_{k-1}(\omega) = \frac{l_{k}(\omega) - \mathcal{L}_{k-1}(\omega)}{k}.$$

\begin{definition}
  Let the Hessian of $\mathcal{L}_k(\omega)$ be:
  $$\mathcal{H}_k(\omega) = \nabla^2_{\omega}\mathcal{L}_k(\omega) = \frac1k \sum\limits_1^k \nabla^2_{\omega} l_i(\omega).$$
\end{definition}

\begin{definition}
  To calculate the overall loss landscape changing, one has to integrate the absolute difference for the entire parameter space.
  We define \textbf{weighted difference} as:
  $$\Delta_{k} = \int \left( \mathcal{L}_{k}(\omega) - \mathcal{L}_{k-1}(\omega) \right)^2 p(\omega) d\omega,$$
  where $p(\omega)$ describes the priority of the particular parameter points so we can make $p(\omega)$ have higher values
  next to the local minima.
\end{definition}

We firther investigate this difference and aimed at exploration of how adding a new object to the dataset changes the value.
We interested in convergence of this value and properties of loss function when the training dataset size limits to $\infty$.


\begin{definition}
  Let $\Delta$ be a positive hyperparameter that indicates the stop-difference for $\Delta_k$. If
  $$\Delta_{k^*} < \Delta \wedge \left( \forall k < k^*: \, \Delta_k \geq \Delta \right),$$
  we can say that $k^*$ samples in the dataset are enough to describe the distribution of data from the general population.
  We call $k^*$ as \textbf{sufficient}.
\end{definition}

\subsection{Assumptions}

\begin{assumption}
  Let $\omega^*$ be the local minimum of both $\mathcal{L}_{k-1}(\omega)$ and $\mathcal{L}_{k}(\omega)$.
  Thus, $$\nabla \mathcal{L}_{k-1}(\omega^*) = \nabla \mathcal{L}_{k}(\omega^*) = 0.$$
\end{assumption}

This assumption allows us to explore the behavior and the geometry of the loss function landscape at only one point.

Furthermore, using second-order Taylor's approximation for $\mathcal{L}_{k}(\omega)$ at $\omega^*$ we get:
$$\mathcal{L}_{k}(\omega) \approx \mathcal{L}_{k}(\omega^*) + \frac12 (\omega - \omega^*)^T \mathcal{H}_k(\omega^*)
  (\omega - \omega^*)$$

\begin{assumption}
  We can assume parameters w to be random, which will lead to quite natural condition: p(w) can be even a prior
  distribution of w, so: $$\Delta_{k} = \mathbb{E}_{p(\omega)} \left( \mathcal{L}_{k}(\omega) - \mathcal{L}_{k-1}(\omega)
    \right)^2 = \mathbb{D} \left( \mathcal{L}_{k}(\omega) - \mathcal{L}_{k-1}(\omega) \right) + \left( \mathbb{E} \left(
      \mathcal{L}_{k}(\omega) - \mathcal{L}_{k-1}(\omega) \right) \right)^2$$
\end{assumption}

\subsection{Transformers and Self-Attention Layers}
To apply the above concepts to transformer architectures, we define the operation of a single self-attention layer, following a generalized approach similar to \cite{ormaniec2024attentionhessian}, which builds on the foundational transformer framework of \cite{vaswani2017attention}. This definition will enable us to analyze the Hessian's structure and its implications for loss landscape convergence as the dataset size increases.

Consider a sequence of token embeddings $X \in \mathbb{R}^{L \times d}$, where $L$ is the sequence length and $d$ is the embedding dimension. A self-attention layer maps $X$ to an output sequence $F(X) \in \mathbb{R}^{L \times d}$ via the following operation:
\[
F(X) = A(X) X W_V,
\]
where $W_V \in \mathbb{R}^{d \times d}$ is the value weight matrix, and $A(X) \in \mathbb{R}^{L \times L}$ is the self-attention matrix, defined as:
\[
A(X) = a(T(X)).
\]
Here, $T(X) \in \mathbb{R}^{L \times L}$ is the query-key similarity transformation, and $a : \mathbb{R}^{L \times L} \to \mathbb{R}^{L \times L}$ is an activation function. In the classical self-attention mechanism \cite{vaswani2017attention}, the similarity transformation is given by:
\[
T(X) = \frac{X W_Q W_K^\top X^\top}{\sqrt{d_K}},
\]
where $W_Q, W_K \in \mathbb{R}^{d \times d_K}$ are the query and key weight matrices, respectively, $d_K$ is the key dimension, and $a = \text{softmax}$ (applied row-wise). This formulation allows for flexibility in exploring variations of the attention mechanism, as discussed in \cite{ormaniec2024attentionhessian}.

The output $F(X)$ is then fed into a loss function $l : \mathbb{R}^{L \times d} \times \mathbb{R}^{L \times d} \to \mathbb{R}$, which measures the discrepancy between the predicted sequence $F(X)$ and the target sequence $Y$. For simplicity, we assume a mean squared error loss, defined as:
\[
l(F(X), Y) = \frac{1}{L d} \| F(X) - Y \|_F^2,
\]
where $\| \cdot \|_F$ denotes the Frobenius norm of a matrix. This loss corresponds to the per-sample loss $l_i(\omega)$ in our general notation, where $\omega$ includes the parameters $\{W_Q, W_K, W_V\}$ of the self-attention layer.

\subsection{Hessian Analysis for Self-Attention Layers}
Our goal is to analyze the Hessian $\mathcal{H}_k(\omega)$ of the empirical loss $\mathcal{L}_k(\omega)$ with respect to the parameters $\omega = \{W_i\}$ of the self-attention layer, where $W_i \in \mathbb{R}^{p_i \times q_i}$ (e.g., $i \in \{Q, K, V\}$ for the query, key, and value weights). Following \cite{ormaniec2024attentionhessian}, the Hessian can be decomposed into blocks $\frac{\partial^2 (\mathcal{L}_k \circ F)}{\partial W_i \partial W_j}$, reflecting the interactions between different parameter matrices. This decomposition is crucial for understanding how the loss landscape evolves as the dataset size $k$ increases.

To facilitate this analysis, we leverage the Gauss-Newton decomposition of the Hessian, a standard technique in neural network analysis \cite{ormaniec2024attentionhessian}. For the composite function $\mathcal{L}_k \circ F$, the Hessian block with respect to parameters $W_i$ and $W_j$ can be expressed as:
\[
\frac{\partial^2 (\mathcal{L}_k \circ F)}{\partial W_i \partial W_j} = \left( \frac{\partial F}{\partial W_i} \right)^\top \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \left( \frac{\partial F}{\partial W_j} \right) + \left( \frac{\partial \mathcal{L}_k}{\partial F} \otimes I_{p_i q_i} \right) \frac{\partial^2 F}{\partial W_i \partial W_j},
\]
where the first term is the outer product Hessian (often related to the Gauss-Newton approximation) and the second term is the functional Hessian, capturing higher-order dependencies in the self-attention mechanism. This decomposition, detailed in \cite{ormaniec2024attentionhessian}, allows us to isolate the contributions of the self-attention matrix $A(X)$ and the value transformation $X W_V$ to the curvature of the loss landscape.

In the context of our study, this Hessian analysis is critical for understanding how $\mathcal{H}_k(\omega)$ changes as new samples are added to the dataset, affecting the weighted difference $\Delta_k$. Specifically, the spectral properties of $\mathcal{H}_k(\omega)$, such as its eigenvalues, provide insights into the flatness or sharpness of the loss landscape.

\section{Method}\label{sec:method}

In this section, we derive generalized Hessian expressions for the self-attention layer and extend them to a full transformer block, leveraging these to analyze the convergence of the loss function surface as the dataset size increases. Our approach builds on the theoretical framework of \cite{ormaniec2024attentionhessian}, adapting and generalizing their results to provide insights into the sufficient dataset size \( k^* \) for transformer architectures.

\subsection{Hessian of the Self-Attention Layer}\label{subsec:hessian_self_attention}

We begin by analyzing the Hessian of a single self-attention layer with parameters \( \omega = \{W_Q, W_K, W_V\} \), where \( W_Q, W_K \in \mathbb{R}^{d \times d_K} \) are the query and key weight matrices, and \( W_V \in \mathbb{R}^{d \times d} \) is the value weight matrix. The input is a sequence of token embeddings \( X \in \mathbb{R}^{L \times d} \), where \( L \) is the sequence length and \( d \) is the embedding dimension. The output of the self-attention layer is:
\[
F(X) = A(X) X W_V,
\]
where \( A(X) = \text{softmax}\left( \frac{X W_Q W_K^\top X^\top}{\sqrt{d_K}} \right) \), and \( d_K \) is the key dimension. The empirical loss is defined as:
\[
\mathcal{L}_k(\omega) = \frac{1}{k} \sum_{i=1}^k l(F(X_i), Y_i),
\]
where \( l(F(X_i), Y_i) \) is a general loss function, not specified here to maintain generality (unlike the mean squared error used in \cite{ormaniec2024attentionhessian}).

The Hessian of \( \mathcal{L}_k \) with respect to the parameters \( \omega \) is:
\[
\mathcal{H}_k(\omega) = \nabla^2_\omega \mathcal{L}_k(\omega) = \frac{1}{k} \sum_{i=1}^k \nabla^2_\omega l_i(\omega),
\]
where \( l_i(\omega) = l(F(X_i), Y_i) \). For parameters \( W_i, W_j \in \{W_Q, W_K, W_V\} \), the Hessian block is decomposed using the Gauss-Newton approximation:
\[
\frac{\partial^2 \mathcal{L}_k}{\partial W_i \partial W_j} = H_o(W_i, W_j) + H_f(W_i, W_j),
\]
with \( H_o \) as the outer-product Hessian and \( H_f \) as the functional Hessian.

\subsubsection{Generalized Outer-Product Hessian \( H_o \)}
The outer-product Hessian captures the second-order effects of the loss with respect to the output \( F \):
\[
H_o(W_i, W_j) = \left( \frac{\partial F}{\partial W_i} \right)^\top \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \left( \frac{\partial F}{\partial W_j} \right),
\]
where \( \frac{\partial F}{\partial W_i} \) is the Jacobian of \( F \) with respect to \( W_i \), and \( \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \in \mathbb{R}^{L \times d \times L \times d} \) is the Hessian of the loss with respect to \( F \), averaged over \( k \) samples. We compute the Jacobians for all parameters:

- \textbf{For \( W_V \):}
\[
\frac{\partial F}{\partial W_V} = A(X) X \otimes I_d,
\]
since \( A(X) \) is independent of \( W_V \), and the derivative is a tensor in \( \mathbb{R}^{L \times d \times d \times d} \).

- \textbf{For \( W_Q \):}
Let \( T(X) = \frac{X W_Q W_K^\top X^\top}{\sqrt{d_K}} \), so \( A(X) = \text{softmax}(T(X)) \). Then:
\[
\frac{\partial F}{\partial W_Q} = \frac{\partial A}{\partial T} \frac{\partial T}{\partial W_Q} X W_V.
\]
The derivative \( \frac{\partial T}{\partial W_Q} = \frac{1}{\sqrt{d_K}} X \otimes (X W_K) \), and \( \frac{\partial A}{\partial T} \) is the Jacobian of the softmax, a fourth-order tensor. Thus:
\[
\frac{\partial F}{\partial W_Q} = \frac{1}{\sqrt{d_K}} \frac{\partial A}{\partial T} (X \otimes X W_K) X W_V.
\]

- \textbf{For \( W_K \):}
Similarly:
\[
\frac{\partial T}{\partial W_K} = \frac{1}{\sqrt{d_K}} (X W_Q) \otimes X,
\]
so:
\[
\frac{\partial F}{\partial W_K} = \frac{1}{\sqrt{d_K}} \frac{\partial A}{\partial T} (X W_Q \otimes X) X W_V.
\]

For each pair \( (W_i, W_j) \), \( H_o \) is computed by contracting the Jacobians with \( \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \). For example:
- \( H_o(W_Q, W_K) = \frac{1}{d_K} \left( (X \otimes X W_K)^\top \frac{\partial A^\top}{\partial T} X W_V \right)^\top \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \left( \frac{\partial A}{\partial T} (X W_Q \otimes X) X W_V \right) \).

The full set of expressions follows this pattern, remaining general without specifying \( \frac{\partial^2 \mathcal{L}_k}{\partial F^2} \).

\subsubsection{Generalized Functional Hessian \( H_f \)}
The functional Hessian accounts for the curvature of \( F \):
\[
H_f(W_i, W_j) = \left( \frac{\partial \mathcal{L}_k}{\partial F} \otimes I_{p_i q_i} \right) \frac{\partial^2 F}{\partial W_i \partial W_j},
\]
where \( \frac{\partial \mathcal{L}_k}{\partial F} \in \mathbb{R}^{L \times d} \) is the gradient of the loss with respect to \( F \), and \( p_i q_i \) is the size of \( W_i \) (e.g., \( d d_K \) for \( W_Q \)). The second derivative \( \frac{\partial^2 F}{\partial W_i \partial W_j} \) varies by pair:

- \textbf{\( H_f(W_V, W_V) \):} Since \( F = A X W_V \) is linear in \( W_V \), \( \frac{\partial^2 F}{\partial W_V \partial W_V} = 0 \), so \( H_f(W_V, W_V) = 0 \).
- \textbf{\( H_f(W_Q, W_K) \):} 
\[
\frac{\partial^2 F}{\partial W_Q \partial W_K} = \frac{1}{d_K} \frac{\partial^2 A}{\partial T^2} \left( (X \otimes X W_K) \otimes (X W_Q \otimes X) \right) X W_V + \frac{1}{\sqrt{d_K}} \frac{\partial A}{\partial T} (X \otimes X),
\]
\[
H_f(W_Q, W_K) = \frac{1}{d_K} \left( \frac{\partial \mathcal{L}_k}{\partial F} \otimes I_{d d_K} \right) \left( \frac{\partial^2 A}{\partial T^2} \left( (X \otimes X W_K) \otimes (X W_Q \otimes X) \right) X W_V + \sqrt{d_K} \frac{\partial A}{\partial T} (X \otimes X) \right).
\]

Other pairs follow similarly, with \( H_f \) non-zero when \( W_i \) and \( W_j \) both influence \( A(X) \).

\subsection{Hessian of the Transformer Block}\label{subsec:hessian_transformer_block}

A transformer block extends the self-attention layer with a feed-forward network (FFN), residual connections, and layer normalization. The output is:
\[
Y = \text{LayerNorm}(X + \text{SelfAttention}(X)),
\]
\[
Z = \text{LayerNorm}(Y + \text{FFN}(Y)),
\]
where \( \text{FFN}(Y) = \sigma(Y W_1 + b_1) W_2 + b_2 \), with \( W_1 \in \mathbb{R}^{d \times d_{\text{ff}}} \), \( W_2 \in \mathbb{R}^{d_{\text{ff}} \times d} \), \( b_1 \in \mathbb{R}^{d_{\text{ff}}} \), \( b_2 \in \mathbb{R}^{d} \), and \( \sigma \) as the activation (e.g., ReLU). The parameters are \( \omega = \{W_Q, W_K, W_V, W_1, W_2, b_1, b_2\} \), assuming fixed layer normalization parameters for simplicity.

\begin{theorem}\label{thm:transformer_hessian}

NEED TO BE COMPLETED TODO

For a transformer block with parameters \( \omega \), the Hessian of the loss \( \mathcal{L}_k \) with respect to parameters \( \omega_i, \omega_j \in \omega \) is:
\[
\mathcal{H}_k = H_o + H_f,
\]
where:
\[
H_o(\omega_i, \omega_j) = \left( \frac{\partial Z}{\partial \omega_i} \right)^\top \frac{\partial^2 \mathcal{L}_k}{\partial Z^2} \left( \frac{\partial Z}{\partial \omega_j} \right),
\]
\[
H_f(\omega_i, \omega_j) = \left( \frac{\partial \mathcal{L}_k}{\partial Z} \otimes I_{p_i q_i} \right) \frac{\partial^2 Z}{\partial \omega_i \partial \omega_j}.
\]
\end{theorem}

The Jacobians \( \frac{\partial Z}{\partial \omega_i} \) are computed via the chain rule through the layers, incorporating derivatives from both the self-attention and FFN components, adjusted for layer normalization. The proof is provided in Appendix~\ref{app:proof_transformer_hessian}.

\subsection{Convergence of the Loss Function Surface}\label{subsec:loss_convergence}

AS IN THE PAPER (NEED REFERENCES)

We apply the Hessian expressions to estimate the convergence of the loss difference \( \Delta_k = \int ( \mathcal{L}_k(\omega) - \mathcal{L}_{k-1}(\omega) )^2 p(\omega) d\omega \). The loss difference is:
\[
\mathcal{L}_{k+1}(\omega) - \mathcal{L}_k(\omega) = \frac{1}{k+1} \left( l_{k+1}(\omega) - \mathcal{L}_k(\omega) \right).
\]
Using a second-order Taylor expansion around a local minimum \( \omega^* \) (where \( \nabla \mathcal{L}_k(\omega^*) = 0 \)):
\[
\mathcal{L}_k(\omega) \approx \mathcal{L}_k(\omega^*) + \frac{1}{2} (\omega - \omega^*)^\top \mathcal{H}_k(\omega^*) (\omega - \omega^*),
\]
\[
\mathcal{L}_{k+1}(\omega) - \mathcal{L}_k(\omega) \approx \frac{1}{2} (\omega - \omega^*)^\top ( \mathcal{H}_{k+1}(\omega^*) - \mathcal{H}_k(\omega^*) ) (\omega - \omega^*).
\]
Thus:
\[
\Delta_{k+1} \approx \mathbb{E}_{p(\omega)} \left[ \left( \frac{1}{2} (\omega - \omega^*)^\top ( \mathcal{H}_{k+1}(\omega^*) - \mathcal{H}_k(\omega^*) ) (\omega - \omega^*) \right)^2 \right].
\]
Substituting \( \mathcal{H}_k = H_o^{(k)} + H_f^{(k)} \) from Theorem~\ref{thm:transformer_hessian}:
\[
\mathcal{H}_{k+1} - \mathcal{H}_k = ( H_o^{(k+1)} - H_o^{(k)} ) + ( H_f^{(k+1)} - H_f^{(k)} ),
\]
where:
\[
H_o^{(k)} = \frac{1}{k} \sum_{i=1}^k \left( \frac{\partial Z_i}{\partial \omega} \right)^\top \frac{\partial^2 l_i}{\partial Z_i^2} \left( \frac{\partial Z_i}{\partial \omega} \right),
\]
\[
H_f^{(k)} = \frac{1}{k} \sum_{i=1}^k \left( \frac{\partial l_i}{\partial Z_i} \otimes I \right) \frac{\partial^2 Z_i}{\partial \omega \partial \omega}.
\]
As \( k \) increases, the difference \( \mathcal{H}_{k+1} - \mathcal{H}_k \) diminishes due to averaging, leading to \( \Delta_k \to 0 \). The rate of convergence depends on the spectral properties of \( H_o \) and \( H_f \), reflecting the transformer block’s architecture and the loss function’s curvature.

\section{Experiments}\label{sec:exp}

To verify the theoretical estimates obtained, we conducted a detailed empirical study. The experiments are divided into two main parts: the base experiment, which demonstrates the general behavior of the loss function convergence, and the transformer-specific experiment, which focuses on the Vision Transformer (ViT) architecture. Both experiments aim to validate the theoretical results and provide insights into the practical implications of our findings.

\subsection{Base Experiment}

The goal of the base experiment is to empirically observe the convergence of the loss function as the dataset size increases. Specifically, we aim to verify whether the difference between the loss functions for consecutive dataset sizes, $|\cL_{k + 1}(\theta) - \cL_{k}(\theta)|$
decreases as $k$ grows, as predicted by our theoretical analysis.

We consider a simple neural network architecture, such as a fully connected network, trained on a synthetic dataset. The dataset is generated from a known distribution to ensure control over the data generation process. The loss function used is the mean squared error (MSE), and the network is trained using stochastic gradient descent (SGD).

The results of the base experiment are presented in Figure~\ref{fig:base_experiment}. The plot shows the convergence of $|\cL_{k + 1}(\theta) - \cL_{k}(\theta)|$ as a function of the dataset size $k$. As expected, the difference decreases monotonically, indicating that the loss function surface stabilizes as the dataset size increases. This behavior aligns with our theoretical predictions, confirming that the loss function converges as the dataset grows.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/mnist_hidden_size.pdf}
    \caption{}
    \label{fig:base_mnist_hidden_size}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/mnist_num_layers.pdf}
    \caption{}
    \label{fig:base_mnist_num_layers}
\end{subfigure}
\caption{Convergence of the loss function difference 
$|\cL_{k + 1}(\theta) - \cL_{k}(\theta)|$ as a function of the dataset size $k$. 
The plots demonstrates the stabilization of the loss function surface as the dataset size increases.}
\label{fig:base_experiment}
\end{figure}
\subsection{Transformer-Specific Experiment}

In this experiment, we focus on the Vision Transformer (ViT) architecture to validate our theoretical results in a more complex and practical setting. The goal is to observe the convergence of the loss function and the quality of classification as the dataset size increases, using a pre-trained ViT model fine-tuned on a small dataset.

We use a pre-trained Vision Transformer model from HuggingFace's transformers library. The model is fine-tuned on a small dataset of images, such as anime characters, cats, or dogs. The fine-tuning process is conducted in two ways:
\begin{enumerate}
\item \textbf{LoRA (Low-Rank Adaptation)}: We apply parameter-efficient fine-tuning using LoRA, which allows us to adapt the model with minimal changes to the original parameters.
\item \textbf{Unfreezing the Last Layers}: We unfreeze the last few layers of the model and attach a classification head, allowing the model to adapt to the specific classification task.
\end{enumerate}

The loss function used is the cross-entropy loss, and the model is trained using the Adam optimizer. We monitor both the classification accuracy and the convergence of the loss function as the dataset size increases.

The results of the transformer-specific experiment will be presented in two parts:
\begin{enumerate}
\item \textbf{Classification Accuracy}: We will report the classification accuracy on a validation set as a function of the dataset size. This will demonstrate how the model's performance improves with more data.
\item \textbf{Loss Function Convergence}: We will plot the difference 
$|\cL_{k + 1}(\theta) - \cL_{k}(\theta)|$ as a function of the dataset size $k$, similar to the base experiment. This will show how the loss function surface stabilizes as the dataset size increases.
\end{enumerate}

\section{Discussion}\label{sec:disc}

TODO

\section{Conclusion}\label{sec:concl}

TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Appendix / supplemental material}\label{app:A}

\subsection{Additional experiments}\label{app:exp}

TODO


\section{Appendix / Proofs of the Theorems}\label{app:B}
\subsection{Proof of Theorem~\ref{thm:transformer_hessian}}\label{app:proof_transformer_hessian}
NEED TO BE COMPLETED TODO
\begin{proof}
Consider the transformer block output \( Z = \text{LayerNorm}(Y + \text{FFN}(Y)) \), where \( Y = \text{LayerNorm}(X + \text{SelfAttention}(X)) \), and the empirical loss \( \mathcal{L}_k(\omega) = \frac{1}{k} \sum_{i=1}^k l(Z_i, Y_i) \). The Hessian is:
\[
\mathcal{H}_k(\omega) = \nabla^2_\omega \mathcal{L}_k(\omega) = \frac{1}{k} \sum_{i=1}^k \nabla^2_\omega l_i(\omega).
\]
For parameters \( \omega_i, \omega_j \in \omega \), apply the chain rule to the composite function \( \mathcal{L}_k \circ Z \):
\[
\frac{\partial^2 \mathcal{L}_k}{\partial \omega_i \partial \omega_j} = \frac{\partial}{\partial \omega_i} \left( \left( \frac{\partial Z}{\partial \omega_j} \right)^\top \frac{\partial \mathcal{L}_k}{\partial Z} \right).
\]
Using the product rule:
\[
\frac{\partial^2 \mathcal{L}_k}{\partial \omega_i \partial \omega_j} = \left( \frac{\partial Z}{\partial \omega_i} \right)^\top \frac{\partial^2 \mathcal{L}_k}{\partial Z^2} \left( \frac{\partial Z}{\partial \omega_j} \right) + \left( \frac{\partial \mathcal{L}_k}{\partial Z} \otimes I_{p_i q_i} \right) \frac{\partial^2 Z}{\partial \omega_i \partial \omega_j}.
\]
- \textbf{Outer-Product Term:} TODO The first term, \( H_o \), arises from the derivative of the loss with respect to \( Z \), requiring the Jacobians \( \frac{\partial Z}{\partial \omega_i} \), computed via:
\[
\frac{\partial Z}{\partial \omega_i} = \frac{\partial \text{LayerNorm}}{\partial (Y + \text{FFN}(Y))} \left( \frac{\partial Y}{\partial \omega_i} + \frac{\partial \text{FFN}}{\partial Y} \frac{\partial Y}{\partial \omega_i} \right),
\]
where \( \frac{\partial Y}{\partial \omega_i} \) involves the self-attention derivatives adjusted for layer normalization.

- \textbf{Functional Term:} TODO The second term, \( H_f \), involves the second derivative of \( Z \), which is non-zero for pairs affecting intermediate layers (e.g., \( W_Q \) and \( W_K \)) and zero when \( Z \) is linear in a parameter (e.g., \( W_2 \)).

This decomposition holds for all parameter pairs, completing the proof.
\end{proof}

\end{document}
